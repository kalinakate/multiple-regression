---
title: "DZ1"
author: "Kate"
date: '11 марта 2018 г '
output: html_document
---

```{r setup, include=FALSE}
library(MASS)
library(nortest)


```

## 1.	Множественная регрессия

У нас данные по Наркоманам. Мои данные: Y-зависимые - уровень тревожности, X-независимые-уровни тяжести.

#Построить уравнение регрессии


Загружаем данные:

```{r}
DataNark <- read.csv("D:/Учеба/8 семестр/Алексеева/DataNark.csv", header = TRUE, sep = ";",dec = ",")
MatX <- read.csv("D:/Учеба/8 семестр/Алексеева/MatrixX.csv", header = TRUE, sep = ";",dec = ",")
```

Тут посмотрим данные:

```{r}
View(DataNark)
```

Но нам то нужны не все данные, удалим ненужные данные

```{r}
MyDataNark <- read.csv("D:/Учеба/8 семестр/Алексеева/MyDataNark.csv", header = TRUE, sep = ";",dec = ",")
MyDataNark
```

```{r}
str(MyDataNark)
```

Посмотрим на зависимую переменную, которую будем предсказывать:

```{r}
hist(MyDataNark$sstati, col="red") #Видим, величина распределена достаточно нормально, небольшой скос влево, размах ее где-то от 20, до 80
```


Построим первую модель, будем предсказывать тревожность, по, для начала, употреблению алкоголя и семейным отношениям: 

```{r}
fit <- lm(sstati~asi3_alc+asi6_soc, data=MyDataNark)
summary(fit) #Значимо предсказывает только семейный статус (asi6_soc)
```

Теперь для всех:

```{r}
fit11 <- lm(sstati~., data=MyDataNark)
summary(fit11) #Видим, что значимо предсказывают тревожность медицинский статус, психиатрический статус, и чуть менее употребление наркотиков и семейные отношения.
```


#Теперь к вопросу о взаимодействии факторов: обсуждаем взаимодействие переменных.
Используем для начала те же 2 переменные: это добавляет в модель третий элемент, который получает свой коэффициент.
Взаимоействие факторов говорит о том, что при изменении уровня одной переменной, вторая начинает влиять на тревожность сильнее, или наоборот

```{r}
fit2 <- lm(sstati~asi3_alc*asi6_soc, data=MyDataNark)
summary(fit2) #Семейное отношение так же значимо предсказывает тревожность. А взаимодействие факторов 3 и 6 оказывается так-же, незначимо.

```


Посмотрим на все остальные:

```{r}
fit22 <- lm(sstati~asi1_med*asi3_alc*asi4_dr*asi5_leg*asi6_soc*asi7_psy, data=MyDataNark)
summary(fit22) #Значимого ничего нет

```

Может, глянуть на взаимодействие значимых:

```{r}
fit3 <- lm(sstati~asi1_med*asi7_psy, data=MyDataNark)
summary(fit3) #Тоже не значимо, по отдельности они значимы, но вот третий элемент оказался незначимым

```

#Приведем еще доверительные интервалы.

```{r}
confint(fit)#Дов.интервалы тут пересекают 0, что значит, они должным образом не предсказывают тревожность, хотя сама переменная семейных отношений выше нуля.
```

Посмотрим на все:

```{r}
confint(fit11)
```


###Строим уравнение регрессии

Будем строить это уравнение матричным способом: (ДЛЯ НАЧАЛА)

```{r}
Y <- matrix(MyDataNark$sstati)

X1 <- matrix(MyDataNark$asi1_med) #Как всегда создаю велосипед
X2 <- matrix(MyDataNark$asi3_alc)
X3 <- matrix(MyDataNark$asi4_dr)
X4 <- matrix(MyDataNark$asi5_leg)
X5 <- matrix(MyDataNark$asi6_soc)
X6 <- matrix(MyDataNark$asi7_psy)
X0 <- matrix(MatX$sstati)
X <- cbind(X0,X1,X2, X3, X4, X5, X6)
X
XT <- t(X)
XTX <- XT%*%X
XTX1 <- ginv(XTX) #Получаем обратную
XTY <- XT%*%Y
B <- XTX1%*%XTY
B # Таким образом, построили уравнение регрессии
XY <- cbind(Y,X1,X2, X3, X4, X5, X6) #Это просто наша матрица вместе с зависимым и независимыми
```

```{r}
# Y=B[1]+B[2]*X[,2]+B[3]*X[,3]+B[4]*X[,4]+B[5]*X[,5]+B[6]*X[,6]+B[7]*X[,7] #Так оно выглядит
```



###Ну тут попробуем по конспекту:

Посчитаем тут коэффициенты lambda. ну то есть первый шаг - вычисляем beta

```{r}
lambda <- matrix(0:48,ncol=7,nrow=7)

for(i in (1:7)){
  for(j in (1:7)){
    lambda[i,j] <- mean(XY[,i]*XY[,j])-mean(XY[,i])*mean(XY[,j])
  }
}
lambda #Матрица симметричная, супер!
```

Давайте вот тут вычислим все эти Аij:

```{r}
A <- matrix(0:48,ncol=7,nrow=7)
lam <- matrix(0:48,ncol=7,nrow=7)

for(i in (1:7)){
  for(j in (1:7)){
    if(i%%2!=0){
      
      if(j%%2!=0){
          lam <- lambda[-i,-j]
          A[i,j] <- det(lam)
      }
      
      if(j%%2==0){
        lam <- lambda[-i,-j]
        A[i,j] <- -det(lam)
      }
      
    }
    
    if(i%%2==0){
      
       if(j%%2!=0){
          lam <- lambda[-i,-j]
          A[i,j] <- -det(lam)
       }
      
      if(j%%2==0){
        lam <- lambda[-i,-j]
        A[i,j] <- det(lam)
      }
      
    }
    
  }
}
A #УРАААААА, получииилось....
```

Теперь посчитаем все beta, чтоб сравнить с полученными матричным способом:

```{r}
#Тут собственно расчитываем все коэффициенты и получаем наше уравнение множественной регрессии
beta <- c(0,0,0,0,0,0,0)
beta[1] <- mean(XY[,1])
beta[2] <- -A[1,2]/A[1,1]
beta[3] <- -A[1,3]/A[1,1]
beta[4] <- -A[1,4]/A[1,1]
beta[5] <- -A[1,5]/A[1,1]
beta[6] <- -A[1,6]/A[1,1]
beta[7] <- -A[1,7]/A[1,1]
beta[1] <- beta[1]-beta[2]*mean(XY[,2])-beta[3]*mean(XY[,3])-beta[4]*mean(XY[,4])-beta[5]*mean(XY[,5])-beta[6]*mean(XY[,6])-beta[7]*mean(XY[,7])
beta #СОВПАЛО!!! 

```

###Множественный коэффициент корелляции:

Тут, для начала посчитаем все нужные нам ro: (коэффициенты корреляции)
```{r}
#получим вектор наших РОшек(частные коэффициенты корреляции:

ro1n_n <- c()
ro1n_n[1]<-0 #Он нам как-бы не нужен
for(i in (2:7)){
  ro1n_n[i] <- -(A[1,i]/(A[1,1]*A[i,i])^(1/2))
}
 ro1n_n
 
 #Посчитаем сигмy:
 #Для начала A, как определитель матрицы lambda
 Adet <- det(lambda)
 sigm1_n_2 <- Adet/A[1,1] #Сигма в квадрате
 sigm1_n_2
 
#Коэффициент детерминации:
 #Сигма1^2=lambda[1,1]
 sigm1_2 <- lambda[1,1]
 sigm1_2
R2 <- (sigm1_2-sigm1_n_2)/sigm1_2
#Тут выведем сам R2(коэффициент детерминации)
R2
#И надо корень от него взять
Sig <- R2^(1/2)
print("Множественный коэффициент корелляции:")
Sig

```

###построить частные коэффициенты корреляции:

```{r}
#А я их уже построила на предыдущем шаге:))))) Катя-молодец)))
 RO <- ro1n_n[-1] 
RO #[1]-частный коэффициент RO_12.34567 и т.д.
```

###Проверка на нормальность остатков:

```{r}
shapiro.test(RO)
```

Еще разные тесты:

```{r}
lillie.test(RO) #тест Колмогорова-Смирнова в модификации Лиллиефорса
pearson.test(RO) #критерий хи-квадрат Пирсона
```



```{r}
X1n <- matrix(MyDataNark$asi1_med) #Как всегда создаю велосипед
X2n <- matrix(MyDataNark$asi3_alc)
X3n <- matrix(MyDataNark$asi4_dr)
X4n <- matrix(MyDataNark$asi5_leg)
X5n <- matrix(MyDataNark$asi6_soc)
X6n <- matrix(MyDataNark$asi7_psy)
Xn <- cbind(X1,X2, X3, X4, X5, X6)

neta <- Xn[,1]-beta[1]*Xn[,2]-beta[2]*Xn[,3]-beta[3]*Xn[,4]-beta[4]*Xn[,5]-beta[5]*Xn[,6]
names(neta)=c()

shapiro.test(neta)
```

p-value < 0.05, значит отвергаем нулевую гипотезу о нормальности остатков.

```{r}
plot(ecdf(neta), xlim=c(-25,5))
curve(pnorm, -25, 5)
```

```{r}
n <- length(neta)
qqnorm(neta)
qqline(qnorm((seq(n)-0.5)/n), col="red")
lines(qnorm((seq(n)-0.5)/n), col="red")
```

```{r}
n <- length(neta)
qnorm((seq(n)-0.5)/n)
```

```{r}
plot(qnorm((seq(n)-0.5)/n))
```

###Пробую на данных Примера из конспекта:  (ЭТО МОЖНО НЕ СМОТРЕТЬ)

```{r}
q1 <- c(68.9,68.1,67.6,69.2,69.2,64.6,67)
q2 <- c(2372,2372,2489,3379,4130,1171,689)
q3 <- c(25.3,28,30,23.5,18,38.4,29.6)
Q <- cbind(q1,q2,q3)
Q

```

```{r}
lambdaA <- matrix(0:8,ncol=3,nrow=3)

for(i in (1:3)){
  for(j in (1:3)){
    lambdaA[i,j] <- mean(Q[,i]*Q[,j])-mean(Q[,i])*mean(Q[,j])
  }
}
lambdaA 
det(lambdaA)

```

теперь A[i,j]:


```{r}
#Делаем проще функцию эту:

AA2 <- matrix(0:8,ncol=3,nrow=3)
lam2 <- matrix(0:8,ncol=3,nrow=3)

for(i in (1:3)){
  for(j in (1:3)){
    if(i%%2!=0){
      
      if(j%%2!=0){
          lamd <- lambdaA[-i,-j]
          AA2[i,j] <- det(lamd)
      }
      
      if(j%%2==0){
        lamd <- lambdaA[-i,-j]
        AA2[i,j] <- -det(lamd)
      }
      
    }
    
    if(i%%2==0){
      
       if(j%%2!=0){
          lamd <- lambdaA[-i,-j]
          AA2[i,j] <- -det(lamd)
       }
      
      if(j%%2==0){
        lamd <- lambdaA[-i,-j]
        AA2[i,j] <- +det(lamd)
      }
      
    }
    
  }
}
AA2
```

Считаем beta:

```{r}
beta2 <- c()
beta2[1] <- mean(Q[,1])
beta2[2] <- -AA2[1,2]/AA2[1,1]
beta2[3] <- -AA2[1,3]/AA2[1,1]
beta2[1] <- beta2[1]-beta2[2]*mean(Q[,2])-beta2[3]*mean(Q[,3])
beta2
```

Коэффициенты ro:

```{r}
ro12_3 <- -(AA2[1,2]/(AA2[1,1]*AA2[2,2])^(1/2))
ro12_3
ro13_2 <- -(AA2[1,3]/(AA2[1,1]*AA2[3,3])^(1/2))
ro13_2
```
Считаем сигму:

```{r}
sig1_2 <- lambdaA[1,1]
sig1_2 
AdetA <- det(lambdaA)
sig1_3_2 <- AdetA/AA2[1,1] #Сигма в квадрате
sig1_3_2
R2A <- (sig1_2-sig1_3_2)/sig1_2
R2A
R2A2 <- R2A^(1/2)
R2A2
```
